{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 2 in Assessment 1\n",
    "#### Student Name: LICUN LIU\n",
    "#### Student ID: 30901235\n",
    "\n",
    "Date: 31/08/2020\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 2.7.11 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include the main libraries you used in your assignment here, e.g.,:\n",
    "* pandas (for dataframe, included in Anaconda Python 2.7) \n",
    "* re (for regular expression, included in Anaconda Python 2.7) \n",
    "* numpy (for numpy array, included in Anaconda Python 2.7) \n",
    "\n",
    "<font size=3, color=\"red\"> Note: this is a sample notebook only. You will need to fill in the proper markdown and code blocks. You might also want to make necessary changes to the structure to meet your own needs. It is important to make sure the logic of how you finish the assessment is clearly shown in this notebook! </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langid in d:\\anaconda\\lib\\site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from langid) (1.18.1)\n"
     ]
    }
   ],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "! pip install langid\n",
    "\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import langid\n",
    "import nltk \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parse Excel files with Pandas\n",
    "\n",
    "In this section, you can write your python scripts to parse the correspondiing file.\n",
    "You should \n",
    "* write proper notes for all code block in this notebook using the Markdown cells\n",
    "* provide proper comment in your scripts\n",
    "* run all cells to make sure scripts are runable. If the scripts cannot be run by the assessors, they will not be assessed and zero mark will be given to the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data = pd.ExcelFile('30901235.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a stop word list and read all words in the file\n",
    "Stop word list: Context-dependent(more than 60 days and rare tokens)\n",
    "All word list: Read all the documents and get a list of all the words\n",
    "\n",
    "### Read all data in the .xlsx and save them as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create some variables\n",
    "stop_data = pd.DataFrame(columns = [\"text\",\"id\",\"created_at\"])\n",
    "new_stop_list = []\n",
    "word_appear  = {}\n",
    "all_words = []\n",
    "\n",
    "\n",
    "# Read the content from all excel forms\n",
    "for data in excel_data.sheet_names:\n",
    "    # Using stop_df list to save the content\n",
    "    stop_df = excel_data.parse(data)\n",
    "    stop_df = stop_df.dropna(axis=1, how='all')\n",
    "    stop_df = stop_df.dropna(axis=0, how='all')\n",
    "    stop_df.columns = [\"text\",\"id\",\"created_at\"]\n",
    "    stop_df = stop_df.drop(stop_df.index[0])\n",
    "    stop_df.index = range(len(stop_df.index))\n",
    "\n",
    "    # Only keeps the tweets that are in English language and Reset the row indices\n",
    "    stop_df_index = []\n",
    "    for index in range(len(stop_df.index)):\n",
    "        if langid.classify(str(stop_df.loc[index, 'text']))[0] != 'en':\n",
    "            stop_df_index.append(index)\n",
    "    for i in stop_df_index:\n",
    "        stop_df.drop(i,0,inplace = True)\n",
    "    stop_df.index = range(len(stop_df.index))\n",
    "    \n",
    " \n",
    "\n",
    "    # Get the word token of all the content\n",
    "    i=0\n",
    "    from nltk.tokenize import RegexpTokenizer \n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "    unigram_tokens = []\n",
    "    \n",
    "    while i < len(stop_df.index):\n",
    "        raw_text = str(stop_df.loc[i, 'text']).lower()\n",
    "        unigram_tokens =  unigram_tokens + tokenizer.tokenize(raw_text) \n",
    "        # Using a space to separate the words in different tweets\n",
    "        unigram_tokens.append(\" \")\n",
    "        i = i+1\n",
    "    \n",
    "    unique_stem_list = list(set(unigram_tokens))\n",
    "    \n",
    "    # Using all_words list to save all word tokens\n",
    "    all_words = all_words + unigram_tokens\n",
    "    \n",
    "    # Using word_appear dictionary to count the number of occurrences of each word token\n",
    "    for i in unique_stem_list:\n",
    "        if i in word_appear:\n",
    "            word_appear[i] = word_appear[i] + 1\n",
    "        else:\n",
    "            word_appear[i] = 1\n",
    "        \n",
    "        # Save the tokens with the length less than 3 in the stop list\n",
    "        if len(i)<3:\n",
    "            new_stop_list.append(i)\n",
    "\n",
    "new_stop = new_stop_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the context-dependent and rare tokens in the stop list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using for loop to find context-dependent and rare tokens\n",
    "for i in word_appear:\n",
    "    if (word_appear[i] < 5) or (word_appear[i] > 60):\n",
    "        new_stop_list.append(i)\n",
    "\n",
    "# Save in the stop list\n",
    "new_stop_list = list(set(new_stop_list))\n",
    "new_stop_list.sort()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using PMI measure to find first 200 meaningful bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(all_words)\n",
    "abc = finder.nbest(bigram_measures.pmi, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save first 200 meaningful bigrams in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bli_list = []\n",
    "i=0\n",
    "while i < len(abc):\n",
    "    bli_list.append(abc[i][0]+'_'+abc[i][1])\n",
    "    i = i +1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words Removal\n",
    "Using stiowirds_en.txt and the stop_word list to remove the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords_list_570 = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords_list_570 = f.read().splitlines()\n",
    "\n",
    "stopped_tokens = [w for w in all_words if w not in stopwords_list_570]\n",
    "stopped_tokens = [w for w in stopped_tokens if w not in new_stop]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens should be stemmed using the Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stem_list = []\n",
    "for i in stopped_tokens:\n",
    "    word = porter.stem(i)\n",
    "    stem_list.append(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put stemmed tokens and first 200 meaningful bigrams together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_list = stem_list + bli_list\n",
    "unique_stem_list = list(set(stem_list))\n",
    "unique_stem_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Output the answer of 30901235_vocab in a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = list(range(0,len(unique_stem_list)))\n",
    "dic = dict(zip(unique_stem_list,nlist))\n",
    "\n",
    "fw = open(\"30901235_vocab.txt\",'w+')\n",
    "\n",
    "count = 0\n",
    "for i in dic:   \n",
    "    fw.write(i + \":\"+ str(count) + '\\n')\n",
    "    count =count + 1\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create countVec, 100uni and bi_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = [\"text\",\"id\",\"created_at\"]) \n",
    "for data in excel_data.sheet_names:\n",
    "    \n",
    "    ### Drop NULL columns and rows\n",
    "    ### Remove all the rows and columns that are empty, i.e., only contains NaNs.\n",
    "    dframe = excel_data.parse(data)\n",
    "    dframe = dframe.dropna(axis=1, how='all')\n",
    "    dframe = dframe.dropna(axis=0, how='all')\n",
    "    dframe.columns = [\"text\",\"id\",\"created_at\"]\n",
    "    dframe = dframe.drop(dframe.index[0])\n",
    "    df = dframe\n",
    "    \n",
    "    ### Reset the row indices with a list of integers.\n",
    "    df.index = range(len(df.index))\n",
    "    \n",
    "    ### Only keeps the tweets that are in English language and Reset the row indices\n",
    "    del_index = []\n",
    "    for index in range(len(df.index)):\n",
    "        if langid.classify(str(df.loc[index, 'text']))[0] != 'en':\n",
    "            del_index.append(index)\n",
    "    for index in del_index:\n",
    "        df.drop(index,0,inplace = True)\n",
    "    df.index = range(len(df.index))\n",
    "    \n",
    "    # Word Tokenization\n",
    "    i=0\n",
    "    from nltk.tokenize import RegexpTokenizer \n",
    "    tokenizer = RegexpTokenizer(\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "    unigram_tokens = []\n",
    "\n",
    "    while i < len(df.index):\n",
    "        raw_text = str(df.loc[i, 'text']).lower()\n",
    "        unigram_tokens =  unigram_tokens + tokenizer.tokenize(raw_text) \n",
    "        i = i+1\n",
    "    \n",
    "\n",
    "    # Create top 100 most frequent bigrams\n",
    "    from nltk.probability import *\n",
    "    from nltk.util import ngrams\n",
    "    bigrams = ngrams(unigram_tokens, n = 2)\n",
    "    fdbigram = FreqDist(bigrams)\n",
    "    \n",
    "    daily_bigram = fdbigram.most_common(100) \n",
    "    \n",
    "    # Save top 100 most frequent bigrams in 30901235_100bi\n",
    "    fi = open(\"30901235_100bi.txt\", 'a+')\n",
    "    fi.write(data + ':')\n",
    "\n",
    "    fi.write(str(daily_bigram))\n",
    "    fi.write('\\n')\n",
    "    fi.close()\n",
    "    \n",
    "    ## Stop Words Removal\n",
    "    ### Using stiowirds_en.txt and my stop list to remove the stop words\n",
    "    import nltk\n",
    "    stopwords_list_570 = []\n",
    "    with open('./stopwords_en.txt') as f:\n",
    "        stopwords_list_570 = f.read().splitlines()\n",
    "\n",
    "    stopped_tokens = [w for w in unigram_tokens if w not in stopwords_list_570]\n",
    "    \n",
    "    stopped_token = [w for w in stopped_tokens if w not in new_stop_list]\n",
    "    \n",
    "    ## Tokens should be stemmed using the Porter stemmer\n",
    "    from nltk.stem import PorterStemmer\n",
    "    porter = PorterStemmer()\n",
    "    stem_list = []\n",
    "    for i in stopped_token:\n",
    "        word = porter.stem(i)\n",
    "        stem_list.append(word)\n",
    "    \n",
    "    ## Creating Count Vectors\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\") \n",
    "    \n",
    "    from nltk.probability import *\n",
    "    fd_1 = FreqDist(stem_list)\n",
    "    most_common = fd_1.most_common()\n",
    "    \n",
    "    ### Output the answer of count_vec in a txt file\n",
    "    files = open(\"30901235_countVec.txt\",'a+')\n",
    "    files.write(data + ',')\n",
    "    for i in most_common:\n",
    "        if i[0] in dic:\n",
    "            files.write(str(dic[i[0]]) + \":\" + str(i[1]) + \",\")\n",
    "    files.write('\\n')   \n",
    "    files.close()\n",
    "    \n",
    "    \n",
    "    # Create first 100 most comman unigrams\n",
    "    most_common_100 = fd_1.most_common(100)\n",
    "    \n",
    "    # Save top 100 most frequent unigrams in 30901235_100uni\n",
    "    fil = open(\"30901235_100uni.txt\", 'a+')\n",
    "    fil.write(data + ':')\n",
    "\n",
    "    fil.write(str(most_common_100))\n",
    "    fil.write('\\n')\n",
    "    fil.close()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
